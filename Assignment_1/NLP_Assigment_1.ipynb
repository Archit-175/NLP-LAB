{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8Jd4OI//vsofFo57+B+Q4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Archit-175/NLP-LAB/blob/main/NLP_Assigment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "884f77b3"
      },
      "source": [
        "# Task\n",
        "Using the Gujarati subset of the \"ai4bharat/IndicCorpV2\" dataset, \\\n",
        "create a sentence tokenizer and a word tokenizer. \\\n",
        "The sentence tokenizer should split each paragraph into sentences. \\\n",
        "The word tokenizer should then split each sentence into words, and it must be able to tokenize punctuation, URLs, numbers (including decimals), email addresses, and dates. \\\n",
        "After tokenizing the data, save the tokenized output into one or more files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2a95d6f"
      },
      "source": [
        "## Load the dataset for Gujarati\n",
        "\n",
        "### Subtask:\n",
        "Load the Gujarati subset of the `ai4bharat/IndicCorpV2` dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46940a32"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the `load_dataset` function and load the specified dataset into a variable named `dataset`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8341f0fb"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://datasets-server.huggingface.co/first-rows?dataset=ai4bharat%2FIndicCorpV2&config=indiccorp_v2&split=guj_Gujr\"\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    # Extract the 'text' value from the 'row' column\n",
        "    df = pd.DataFrame([row['row'] for row in data['rows']])\n",
        "    # Remove empty rows\n",
        "    df['text'].replace('', np.nan, inplace=True)\n",
        "    df.dropna(subset=['text'], inplace=True)\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(f\"Failed to download data. Status code: {response.status_code}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bee0b71"
      },
      "source": [
        "import re\n",
        "\n",
        "def sentence_tokenizer(text):\n",
        "    # Split sentences based on Gujarati sentence terminators\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|ред)\\s', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "def word_tokenizer(sentence):\n",
        "    # Regex to capture URLs, email addresses, dates, numbers (including decimals), and words/punctuation\n",
        "    pattern = re.compile(r'[\\w\\.-]+@[\\w\\.-]+|\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b\\d+\\.\\d+\\b|\\b\\d+\\b|[\\u0A80-\\u0AFF]+|[^\\s\\w]+')\n",
        "    return pattern.findall(sentence)\n",
        "\n",
        "df['sentences'] = df['text'].apply(sentence_tokenizer)\n",
        "df['words'] = df['sentences'].apply(lambda sentences: [word_tokenizer(sentence) for sentence in sentences])\n",
        "\n",
        "print(df[['text', 'sentences', 'words']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "474624d6"
      },
      "source": [
        "with open('tokenized_gujarati_text.txt', 'w', encoding='utf-8') as f:\n",
        "    for sentences in df['words']:\n",
        "        for sentence in sentences:\n",
        "            f.write(' '.join(sentence) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ebfad5"
      },
      "source": [
        "total_sentences = df['sentences'].apply(len).sum()\n",
        "print(f\"Total number of sentences: {total_sentences}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db37607d"
      },
      "source": [
        "total_words = df['words'].apply(lambda sentences: sum(len(sentence) for sentence in sentences)).sum()\n",
        "print(f\"Total number of words: {total_words}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "406ac65e"
      },
      "source": [
        "total_characters = df['text'].apply(len).sum()\n",
        "print(f\"Total number of characters: {total_characters}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edf08c26"
      },
      "source": [
        "average_sentence_length = total_words / total_sentences\n",
        "average_word_length = total_characters / total_words\n",
        "\n",
        "print(f\"Average Sentence Length: {average_sentence_length:.2f} words per sentence\")\n",
        "print(f\"Average word length: {average_word_length:.2f} characters per word\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23498353"
      },
      "source": [
        "# Calculate Type-Token Ratio (TTR)\n",
        "all_words = [word for sentences in df['words'] for sentence in sentences for word in sentence]\n",
        "unique_words = set(all_words)\n",
        "ttr = len(unique_words) / len(all_words)\n",
        "\n",
        "print(f\"Type/Token Ratio (TTR): {ttr:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5bVDHoxmySeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hindi\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zX49EDEWh6qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"ai4bharat/IndicCorpV2\",\n",
        "    name=\"indiccorp_v2\",\n",
        "    split=\"hin_Deva\",\n",
        "    streaming=True  # Optional: avoid full download\n",
        ")\n",
        "\n",
        "print(next(iter(dataset)))"
      ],
      "metadata": {
        "id": "Jzergymhb9rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_sentence_tokenizer(text):\n",
        "    # Split by Hindi sentence enders: ред ? !\n",
        "    # Keep punctuation attached to sentence\n",
        "    sentences = re.split(r'(?<=[ред!?])\\s+', text.strip())\n",
        "    return [s.strip() for s in sentences if s.strip()]"
      ],
      "metadata": {
        "id": "AgFbj-MRiBbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def custom_word_tokenizer(text):\n",
        "    pattern = (\n",
        "        r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b'       # emails\n",
        "        r'|https?://\\S+'                    # URLs\n",
        "        r'|\\d+\\.\\d+|\\d+'                    # numbers\n",
        "        r'|[\\u0900-\\u097F]+'                # Hindi words (matras included)\n",
        "        r'|[^\\s\\w\\u0900-\\u097F]'            # punctuation\n",
        "    )\n",
        "    return re.findall(pattern, text)"
      ],
      "metadata": {
        "id": "R57Q0om-iNgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ai4bharat/IndicCorpV2\", name=\"indiccorp_v2\", split=\"hin_Deva\", streaming=True)\n",
        "\n",
        "total_sentences = 0\n",
        "total_words = 0\n",
        "total_chars = 0\n",
        "unique_words = set()\n",
        "\n",
        "MAX_PARAGRAPHS = 500_000  # Optional limit for speed\n",
        "\n",
        "for i, example in enumerate(dataset):\n",
        "    text = example[\"text\"]\n",
        "\n",
        "    # Custom sentence tokenizer\n",
        "    sentences = custom_sentence_tokenizer(text)\n",
        "    for sent in sentences:\n",
        "        total_sentences += 1\n",
        "        words = custom_word_tokenizer(sent)\n",
        "        total_words += len(words)\n",
        "        total_chars += sum(len(word) for word in words)\n",
        "        unique_words.update(words)\n",
        "\n",
        "    if i + 1 >= MAX_PARAGRAPHS:\n",
        "        break\n",
        "\n",
        "# Final statistics\n",
        "print(f\"Total sentences: {total_sentences}\")\n",
        "print(f\"Total words: {total_words}\")\n",
        "print(f\"Total characters: {total_chars}\")\n",
        "print(f\"Average sentence length (words/sentence): {total_words / total_sentences:.2f}\")\n",
        "print(f\"Average word length (characters/word): {total_chars / total_words:.2f}\")\n",
        "print(f\"Type/Token Ratio: {len(unique_words) / total_words:.4f}\")"
      ],
      "metadata": {
        "id": "PMnWepouixGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cfe0b05"
      },
      "source": [
        "import re\n",
        "\n",
        "def find_gujarati_words(text):\n",
        "    return re.findall(r'[\\u0A80-\\u0AFF]+', text)\n",
        "\n",
        "text = \"This is a test with some Gujarati words: ркЧрлБркЬрк░рк╛ркдрлА рк╢ркмрлНркжрлЛ\"\n",
        "gujarati_words = find_gujarati_words(text)\n",
        "print(gujarati_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1d565a2",
        "outputId": "f6318a87-59c8-43d8-8a2b-3a4840455daf"
      },
      "source": [
        "text = \"ркХрлЗрко ркЫрлЛ? and ркоркЬрк╛ркорк╛.\"\n",
        "gujarati_words = find_gujarati_words(text)\n",
        "print(gujarati_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ркХрлЗрко', 'ркЫрлЛ', 'ркоркЬрк╛ркорк╛']\n"
          ]
        }
      ]
    }
  ]
}
